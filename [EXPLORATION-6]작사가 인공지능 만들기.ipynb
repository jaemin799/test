{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20dbb516",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4769cf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 개수 :  49\n",
      "파일 경로 :  /aiffel/data/lyrics/janisjoplin.txt\n"
     ]
    }
   ],
   "source": [
    "# 해당 경로에 있는 모든 파일을 불러오기\n",
    "txt_file_path = os.getenv('HOME')+'/data/lyrics/*'\n",
    "\n",
    "# glob.glob() 함수는 파라미터에 명시된 저장 경로와 패턴에 해당하는 파일명을 리스트 형식으로 반환한다.\n",
    "# 참고 - https://velog.io/@dkwjd131/Python-glob.glob-%ED%95%A8%EC%88%98-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "print(\"파일 개수 : \",len(txt_list))\n",
    "print(\"파일 경로 : \",txt_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8dd642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "샘플:\n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained']\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        \n",
    "        # splitlines()는 가져온 파일의 모든 문자열을 하나의 문자열로 만들어줌\n",
    "        raw = f.read().splitlines()\n",
    "        \n",
    "        #raw_corpus.append(raw)\n",
    "        #이렇게 하니까 모든 문자 내용이 나옴. 슬라이스로 범위가 정해졌을 텐데, 왜 그럴까요?\n",
    "        \n",
    "        raw_corpus.extend(raw)\n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"샘플:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6066614",
   "metadata": {},
   "source": [
    "# 데이터 가공하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e07041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 역물음표는 의무문의 첫부분에 사용한다. 문장 중간에도 사용할 수 있다. 이때는 의무문이 시작되는 구역을 말해준다.\n",
    "    # 참고 자료 - 위키백과\n",
    "    sentence = re.sub(r'([?.!,¿])', r' \\1 ', sentence)\n",
    "    \n",
    "    # 부호 더하기는 무엇을 의미하는가?\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    sentence = re.sub(r'[^a-zA-Z?.!,¿]+', \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb190fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['busted flat in baton rouge , waitin for a train',\n",
       " 'and i s feelin near as faded as my jeans',\n",
       " 'bobby thumbed a diesel down , just before it rained']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "235e0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116823\n"
     ]
    }
   ],
   "source": [
    "# 사용한 단어를 보려고요.\n",
    "# 참고 - https://webisfree.com/2017-11-20/python-list%EA%B0%92-unique%ED%95%9C-%EA%B3%A0%EC%9C%A0%EA%B0%92%EB%A7%8C-%EA%B0%80%EC%A7%80%EB%8A%94-%EB%B0%A9%EB%B2%95\n",
    "unique_corpus = set(corpus)\n",
    "print(len(unique_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b062f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3606 1689   11 ...    0    0    0]\n",
      " [   5    2   13 ...    0    0    0]\n",
      " [ 801 7661    6 ...    0    0    0]\n",
      " ...\n",
      " [   2   19  696 ...    0    0    0]\n",
      " [   2   19  696 ...    0    0    0]\n",
      " [   2   19  696 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f3a55aa80d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=116823, filters=' ')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f470f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3606 1689   11 7055 3992    1 1097   25    6  678]\n",
      " [   5    2   13  510  845   78 2586   78   10  945]\n",
      " [ 801 7661    6 6045   57    1   33  182    8 4572]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905b5aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : ,\n",
      "2 : i\n",
      "3 : the\n",
      "4 : you\n",
      "5 : and\n",
      "6 : a\n",
      "7 : to\n",
      "8 : it\n",
      "9 : me\n",
      "10 : my\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcb6d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3606 1689   11 7055 3992    1 1097   25    6  678    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "[1689   11 7055 3992    1 1097   25    6  678    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c12f1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 344), (256, 344)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "021c5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words+1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26c161c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[88064,116824] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/44147886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msrc_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_sample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/4071661617.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;31m# Broadcast kernel to inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m       \u001b[0;31m# Reshape the output back to the original ndim of the input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes, name)\u001b[0m\n\u001b[1;32m   5040\u001b[0m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001b[1;32m   5041\u001b[0m         b, b_axes, True)\n\u001b[0;32m-> 5042\u001b[0;31m     \u001b[0mab_matmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_reshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5043\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5044\u001b[0m       if (ab_matmul.get_shape().is_fully_defined() and\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3652\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[1;32m   3653\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3655\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[1;32m   3656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5694\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5695\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5696\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5697\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5698\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[88064,116824] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "    \n",
    "model(src_sample)\n",
    "\n",
    "# 다 읽지 않았으나 \n",
    "# https://iambeginnerdeveloper.tistory.com/69\n",
    "# 여기서 대충 읽어보면 입력 데이터의 사이즈를 줄여보라고 합니다\n",
    "# 그래서 줄이기로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19a86cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3606 1689   11 ...    0    0    0]\n",
      " [   5    2   13 ...    0    0    0]\n",
      " [ 801    6 6045 ...    0    0    0]\n",
      " ...\n",
      " [   2   19  696 ...    0    0    0]\n",
      " [   2   19  696 ...    0    0    0]\n",
      " [   2   19  696 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f3a4f5cc5e0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 단어 개수를 LSM에 나오는 대로 7천개로 수정합니다.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=7000, filters=' ')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a80406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : ,\n",
      "2 : i\n",
      "3 : the\n",
      "4 : you\n",
      "5 : and\n",
      "6 : a\n",
      "7 : to\n",
      "8 : it\n",
      "9 : me\n",
      "10 : my\n"
     ]
    }
   ],
   "source": [
    "# 다시 코드를 쓰고\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break\n",
    "        \n",
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words+1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfb445d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 341, 7001), dtype=float32, numpy=\n",
       "array([[[ 1.22533529e-04,  2.53056380e-04,  3.58988153e-04, ...,\n",
       "         -1.65925856e-04, -1.43689913e-05,  5.41610780e-06],\n",
       "        [ 1.98013913e-05,  3.75200296e-04,  5.86441893e-04, ...,\n",
       "          5.58174543e-06,  1.93049578e-04,  1.70681684e-04],\n",
       "        [-2.57018226e-04,  5.28614735e-04,  8.82467022e-04, ...,\n",
       "          2.98110797e-04,  1.40572040e-04,  1.87408714e-05],\n",
       "        ...,\n",
       "        [ 2.40815469e-04, -1.06788275e-05, -3.40687315e-04, ...,\n",
       "          2.13755717e-04,  4.18205280e-03,  4.53956984e-03],\n",
       "        [ 2.40815818e-04, -1.06780999e-05, -3.40687373e-04, ...,\n",
       "          2.13755600e-04,  4.18205233e-03,  4.53957031e-03],\n",
       "        [ 2.40814188e-04, -1.06781436e-05, -3.40686966e-04, ...,\n",
       "          2.13755950e-04,  4.18205187e-03,  4.53957031e-03]],\n",
       "\n",
       "       [[-4.97818110e-05,  1.89638755e-04, -8.17836408e-05, ...,\n",
       "          1.72071872e-04,  4.23543737e-04,  2.21111724e-04],\n",
       "        [-1.07636828e-04,  4.37565061e-04, -1.65887250e-04, ...,\n",
       "          3.72552051e-04,  1.00284675e-03,  5.49856573e-04],\n",
       "        [-1.56892071e-04,  6.73777540e-04, -2.18385598e-04, ...,\n",
       "          5.41783171e-04,  1.59539294e-03,  9.24090098e-04],\n",
       "        ...,\n",
       "        [ 2.40813635e-04, -1.06775078e-05, -3.40687984e-04, ...,\n",
       "          2.13756153e-04,  4.18204907e-03,  4.53957217e-03],\n",
       "        [ 2.40813053e-04, -1.06777552e-05, -3.40688275e-04, ...,\n",
       "          2.13756095e-04,  4.18204954e-03,  4.53957357e-03],\n",
       "        [ 2.40812704e-04, -1.06779444e-05, -3.40687693e-04, ...,\n",
       "          2.13755629e-04,  4.18204907e-03,  4.53957217e-03]],\n",
       "\n",
       "       [[ 1.40796692e-04,  4.93533793e-04,  1.45024227e-04, ...,\n",
       "          1.99408660e-05, -3.03394452e-04,  3.14732199e-04],\n",
       "        [ 2.11621300e-04,  8.46181414e-04,  3.65294720e-04, ...,\n",
       "         -4.01226775e-04, -4.00291261e-04,  1.61056290e-04],\n",
       "        [ 4.99589602e-04,  1.02353236e-03,  4.87075973e-04, ...,\n",
       "         -7.86143763e-04, -2.16632252e-04,  1.98836293e-04],\n",
       "        ...,\n",
       "        [ 2.40815061e-04, -1.06784719e-05, -3.40687082e-04, ...,\n",
       "          2.13756633e-04,  4.18205373e-03,  4.53956937e-03],\n",
       "        [ 2.40814479e-04, -1.06785446e-05, -3.40686209e-04, ...,\n",
       "          2.13757099e-04,  4.18205280e-03,  4.53956984e-03],\n",
       "        [ 2.40814596e-04, -1.06783555e-05, -3.40686616e-04, ...,\n",
       "          2.13756983e-04,  4.18205326e-03,  4.53956937e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-5.70243828e-05, -2.80703243e-04,  2.61895679e-04, ...,\n",
       "         -9.74594950e-05,  8.35291212e-05,  1.57847753e-04],\n",
       "        [-5.20592912e-05, -6.08344330e-04,  4.01188532e-04, ...,\n",
       "          3.77081633e-05,  2.50205427e-04,  1.04582687e-04],\n",
       "        [ 1.67572925e-05, -6.76477677e-04,  4.86862584e-04, ...,\n",
       "         -1.78771777e-04,  2.33759667e-04,  1.56971422e-04],\n",
       "        ...,\n",
       "        [ 2.40813431e-04, -1.06777215e-05, -3.40686383e-04, ...,\n",
       "          2.13756764e-04,  4.18205140e-03,  4.53956891e-03],\n",
       "        [ 2.40813519e-04, -1.06774723e-05, -3.40687693e-04, ...,\n",
       "          2.13756721e-04,  4.18205140e-03,  4.53956798e-03],\n",
       "        [ 2.40814188e-04, -1.06773432e-05, -3.40686849e-04, ...,\n",
       "          2.13756299e-04,  4.18205094e-03,  4.53956798e-03]],\n",
       "\n",
       "       [[ 2.86333263e-04,  7.61937772e-05, -3.19944556e-05, ...,\n",
       "         -1.49930507e-04,  9.73528731e-05,  1.27778447e-04],\n",
       "        [ 2.35711137e-04,  5.23149583e-06,  7.02569814e-05, ...,\n",
       "         -4.34719579e-04, -2.55319937e-05,  2.07348261e-04],\n",
       "        [ 3.84114217e-04, -9.10407107e-05,  1.85121826e-04, ...,\n",
       "         -7.61772739e-04, -2.33579762e-04,  1.73021122e-04],\n",
       "        ...,\n",
       "        [ 2.40812602e-04, -1.06804746e-05, -3.40686995e-04, ...,\n",
       "          2.13755571e-04,  4.18205140e-03,  4.53957124e-03],\n",
       "        [ 2.40813184e-04, -1.06805910e-05, -3.40687169e-04, ...,\n",
       "          2.13755979e-04,  4.18205000e-03,  4.53957124e-03],\n",
       "        [ 2.40812893e-04, -1.06807656e-05, -3.40687518e-04, ...,\n",
       "          2.13756211e-04,  4.18205047e-03,  4.53957310e-03]],\n",
       "\n",
       "       [[-1.32980349e-04,  3.20894993e-04, -2.48378376e-04, ...,\n",
       "          8.49445933e-05,  8.96767524e-05,  3.34474957e-04],\n",
       "        [-8.08149125e-05,  3.90831090e-04, -3.13322089e-04, ...,\n",
       "          2.41250847e-04,  2.98431667e-04,  3.70742870e-04],\n",
       "        [-1.68427232e-05,  4.64461249e-04, -3.67464556e-04, ...,\n",
       "          4.17448988e-04,  4.74079599e-04,  4.73074528e-04],\n",
       "        ...,\n",
       "        [ 2.40815105e-04, -1.06794487e-05, -3.40687868e-04, ...,\n",
       "          2.13756866e-04,  4.18205000e-03,  4.53957170e-03],\n",
       "        [ 2.40815221e-04, -1.06797543e-05, -3.40687460e-04, ...,\n",
       "          2.13756401e-04,  4.18205000e-03,  4.53957077e-03],\n",
       "        [ 2.40815047e-04, -1.06795214e-05, -3.40687751e-04, ...,\n",
       "          2.13756983e-04,  4.18205000e-03,  4.53957031e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "    \n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d9eae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 70/730 [=>............................] - ETA: 37:21 - loss: 0.7357"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0104c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
