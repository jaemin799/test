{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab187ad8",
   "metadata": {},
   "source": [
    "# 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21032b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/aiffel/aiffel/lyricist/data/lyrics/janisjoplin.txt', '/aiffel/aiffel/lyricist/data/lyrics/nursery_rhymes.txt', '/aiffel/aiffel/lyricist/data/lyrics/dickinson.txt', '/aiffel/aiffel/lyricist/data/lyrics/nicki-minaj.txt', '/aiffel/aiffel/lyricist/data/lyrics/disney.txt', '/aiffel/aiffel/lyricist/data/lyrics/bob-marley.txt', '/aiffel/aiffel/lyricist/data/lyrics/beatles.txt', '/aiffel/aiffel/lyricist/data/lyrics/dj-khaled.txt', '/aiffel/aiffel/lyricist/data/lyrics/kanye.txt', '/aiffel/aiffel/lyricist/data/lyrics/patti-smith.txt', '/aiffel/aiffel/lyricist/data/lyrics/drake.txt', '/aiffel/aiffel/lyricist/data/lyrics/r-kelly.txt', '/aiffel/aiffel/lyricist/data/lyrics/rihanna.txt', '/aiffel/aiffel/lyricist/data/lyrics/cake.txt', '/aiffel/aiffel/lyricist/data/lyrics/bruno-mars.txt', '/aiffel/aiffel/lyricist/data/lyrics/leonard-cohen.txt', '/aiffel/aiffel/lyricist/data/lyrics/ludacris.txt', '/aiffel/aiffel/lyricist/data/lyrics/kanye-west.txt', '/aiffel/aiffel/lyricist/data/lyrics/eminem.txt', '/aiffel/aiffel/lyricist/data/lyrics/radiohead.txt', '/aiffel/aiffel/lyricist/data/lyrics/lorde.txt', '/aiffel/aiffel/lyricist/data/lyrics/notorious-big.txt', '/aiffel/aiffel/lyricist/data/lyrics/prince.txt', '/aiffel/aiffel/lyricist/data/lyrics/paul-simon.txt', '/aiffel/aiffel/lyricist/data/lyrics/bieber.txt', '/aiffel/aiffel/lyricist/data/lyrics/lin-manuel-miranda.txt', '/aiffel/aiffel/lyricist/data/lyrics/Kanye_West.txt', '/aiffel/aiffel/lyricist/data/lyrics/joni-mitchell.txt', '/aiffel/aiffel/lyricist/data/lyrics/alicia-keys.txt', '/aiffel/aiffel/lyricist/data/lyrics/nirvana.txt', '/aiffel/aiffel/lyricist/data/lyrics/johnny-cash.txt', '/aiffel/aiffel/lyricist/data/lyrics/missy-elliott.txt', '/aiffel/aiffel/lyricist/data/lyrics/bruce-springsteen.txt', '/aiffel/aiffel/lyricist/data/lyrics/jimi-hendrix.txt', '/aiffel/aiffel/lyricist/data/lyrics/bob-dylan.txt', '/aiffel/aiffel/lyricist/data/lyrics/bjork.txt', '/aiffel/aiffel/lyricist/data/lyrics/dolly-parton.txt', '/aiffel/aiffel/lyricist/data/lyrics/blink-182.txt', '/aiffel/aiffel/lyricist/data/lyrics/michael-jackson.txt', '/aiffel/aiffel/lyricist/data/lyrics/britney-spears.txt', '/aiffel/aiffel/lyricist/data/lyrics/al-green.txt', '/aiffel/aiffel/lyricist/data/lyrics/Lil_Wayne.txt', '/aiffel/aiffel/lyricist/data/lyrics/lil-wayne.txt', '/aiffel/aiffel/lyricist/data/lyrics/dr-seuss.txt', '/aiffel/aiffel/lyricist/data/lyrics/notorious_big.txt', '/aiffel/aiffel/lyricist/data/lyrics/lady-gaga.txt', '/aiffel/aiffel/lyricist/data/lyrics/nickelback.txt', '/aiffel/aiffel/lyricist/data/lyrics/amy-winehouse.txt', '/aiffel/aiffel/lyricist/data/lyrics/adele.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "# 다운받은 파일들을 경로를 지정\n",
    "txt_file_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "#txt_fila_path에 있는 파일들을 리스트 형식으로 저장합니다\n",
    "#glob.glob는 무슨 기능일까요? 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다. 사용할 파일들이 많을 때 유용하다.\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "print(txt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d89d6",
   "metadata": {},
   "source": [
    "# 파일에 있는 말뭉치 꺼내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9fa6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기 :  187088\n",
      "샘플 : \n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained']\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기 : \", len(raw_corpus))\n",
    "print(\"샘플 : \\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c447f7f",
   "metadata": {},
   "source": [
    "# 데이터 정제"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11f48e15",
   "metadata": {},
   "source": [
    "하나 보니까 생각나는데 re.sub의 r은 무슨 의미일까? 그리고 +는 무슨 기능을 하는 걸까?\n",
    "+는 앞의 데이터 중에 하나라도 포함되면이라는 뜻 같다. r은 raw를 의미한다고 한다. \n",
    "\n",
    "strip()의 기능은 무엇일까?\n",
    "strip() 함수는 문자열에서 시작부터 끝에서 가로 안에 내용을 삭제하는 기능이다. 지금은 아무 데이터를 입력하지 않아서 공백을 삭제한다는 의미."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29f9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'([!.?,¿])', r' \\1 ', sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"([^a-zA-Z!.?,¿])+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence +' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e62f1a",
   "metadata": {},
   "source": [
    "처음에 for문에서 토큰 15개 이상 되는 시퀀스를 제거하려고 \n",
    "\n",
    "if preprocessed_sentence > 15: continue\n",
    "\n",
    "이렇게 코드를 넣었다. \n",
    "그랬더니 모든 데이터가 제거되어서 시작 토큰과 끝 토큰만 남게 되었다. 곧 문자 하나하나를 일일이 길이로 카운터 하기 때문이라는 것을 알게 되었다. tf.keras.preprocessing.text.Tokenizer로 말뭉치를 텐서로 변환할 때 15개 토큰을 초과하는 것을 제거하는 것으로 해봐야 되겠다. 단 패딩 작업 이전에는 수행을 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2de347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> busted flat in baton rouge , waitin for a train <end>', '<start> and i s feelin near as faded as my jeans <end>', '<start> bobby thumbed a diesel down , just before it rained <end>']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45d4a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3603, 1686, 13, 7052, 3989, 1094, 26, 8, 675, 2]\n",
      "[[   3 3603 1686 ...    0    0    0]\n",
      " [   3    7    4 ...    0    0    0]\n",
      " [   3  798 7658 ...    0    0    0]\n",
      " ...\n",
      " [   3    4   20 ...    0    0    0]\n",
      " [   3    4   20 ...    0    0    0]\n",
      " [   3    4   20 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f652d5a5b50>\n"
     ]
    }
   ],
   "source": [
    "def Tokenizer(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=12000, oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    print(tensor[0])\n",
    "    tensor_15 = []\n",
    "    for tensor_line in tensor:\n",
    "        if len(tensor_line) <= 15:\n",
    "            tensor_15.append(tensor_line)\n",
    "        else:\n",
    "            continue\n",
    "    tensor_15 = tf.keras.preprocessing.sequence.pad_sequences(tensor_15, padding='post', maxlen=20)\n",
    "    print(tensor_15, tokenizer)\n",
    "    return tensor_15, tokenizer\n",
    "\n",
    "tensor_15, tokenizer = Tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932d7097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3, 3603, 1686,   13, 7052, 3989, 1094,   26,    8,  675,    2,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   3,    7,    4,   15,  507,  842,   77, 2583,   77,   12,  942,\n",
       "           2,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   3,  798, 7658,    8, 6042,   56,   34,  181,   10, 4569,    2,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_15[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ccd6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : end\n",
      "3 : start\n",
      "4 : i\n",
      "5 : the\n",
      "6 : you\n",
      "7 : and\n",
      "8 : a\n",
      "9 : to\n",
      "10 : it\n",
      "11 : me\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    if idx > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ceee95",
   "metadata": {},
   "source": [
    "# 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9aa561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3 3603 1686   13 7052 3989 1094   26    8  675    2    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[3603 1686   13 7052 3989 1094   26    8  675    2    0    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor_15[:, :-1]\n",
    "tga_input = tensor_15[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tga_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22820770",
   "metadata": {},
   "source": [
    "오랜만에 train_test_split를 사용하다 보니까 전반적으로 까먹어서 구글링을 통해서 라이브러리를 불러오는 법을 기억했다.\n",
    "일단 데이터셋을 만들기 전에 훈련데이터와 테스트 데이터로 나누는 것부터 하기록 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8700db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tga_input, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d1ba8",
   "metadata": {},
   "source": [
    "# 데이터셋 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42876033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 19), (256, 19)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데에터를 데이터셋 객체로 생성\n",
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words+1\n",
    "\n",
    "data_train = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "data_train = data_train.shuffle(BUFFER_SIZE)\n",
    "data_train = data_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ad7538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#256 = 시퀀스 데이터 수량, 문장\n",
    "#19 = 시퀀스 길이, 단어\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb55b7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "171e1044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 19), (256, 19)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터를 데이터셋 객체로 생성\n",
    "BUFFER_SIZE = len(enc_val)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words+1\n",
    "\n",
    "data_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "data_val = data_val.shuffle(BUFFER_SIZE)\n",
    "data_val = data_val.batch(BATCH_SIZE, drop_remainder=True)\n",
    "data_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328633d",
   "metadata": {},
   "source": [
    "# 인공지능 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e9909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b534a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerator(tokenizer.num_words+1, 256, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a054f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae179b5",
   "metadata": {},
   "source": [
    "엄청 힘들었어요. 처음에는 훈련데이터셋 따로 훈련하고 검증데이터셋 따로 훈련하는건가? 하고 생각했어요.\n",
    "그러다가 전이학습이 생각이 나서 훈련데이터셋으로 훈련한 다음 추가로 검증데이터셋을 훈련하는 건가하고 생각도 했죠.\n",
    "그런데 아무리 생각해도 EXPLORATION 6번 노드를 진행하기에는 전이학습은 진도가 빠른 감이 있었어요.\n",
    "\n",
    "결국 고민 끝에 웅제님께 물어봤고, 웅제님이 노드에서 링크(https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)를 타고 들어가라고 했어요. 그랬더니 fit 함수에 validation_data=라는 메소드가 있는 거에요. 혹시나 돌렸는데, val_loss도 함께 나와서 너무 좋았어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e59d34f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "536/536 [==============================] - 138s 247ms/step - loss: 2.4232 - val_loss: 2.1522\n",
      "Epoch 2/10\n",
      "536/536 [==============================] - 135s 252ms/step - loss: 2.0799 - val_loss: 2.0514\n",
      "Epoch 3/10\n",
      "536/536 [==============================] - 136s 253ms/step - loss: 1.9791 - val_loss: 1.9706\n",
      "Epoch 4/10\n",
      "536/536 [==============================] - 136s 253ms/step - loss: 1.8937 - val_loss: 1.9160\n",
      "Epoch 5/10\n",
      "536/536 [==============================] - 136s 253ms/step - loss: 1.8206 - val_loss: 1.8696\n",
      "Epoch 6/10\n",
      "536/536 [==============================] - 136s 253ms/step - loss: 1.7552 - val_loss: 1.8326\n",
      "Epoch 7/10\n",
      "536/536 [==============================] - 136s 254ms/step - loss: 1.6937 - val_loss: 1.8021\n",
      "Epoch 8/10\n",
      "536/536 [==============================] - 136s 253ms/step - loss: 1.6364 - val_loss: 1.7772\n",
      "Epoch 9/10\n",
      "536/536 [==============================] - 136s 253ms/step - loss: 1.5830 - val_loss: 1.7559\n",
      "Epoch 10/10\n",
      "536/536 [==============================] - 136s 254ms/step - loss: 1.5328 - val_loss: 1.7389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f651db65f10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_train, epochs=10, validation_data=data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89d22e",
   "metadata": {},
   "source": [
    "# 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30e206a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 함수\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"end\"]\n",
    "    \n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token:break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "            \n",
    "    generated = ''\n",
    "    \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed127153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start i love you end '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c9ee6",
   "metadata": {},
   "source": [
    "마지막에 키에러가 났다. index_word를 통해서 보니까 <end>가 end로 되어 있었다.\n",
    "    \n",
    "그래서 end_token에 end로 고쳤다.\n",
    "    \n",
    "이 프로젝트를 잘 수행하기 위해서 총 3일 정도 공부했다. 금,일,월 그리고 화요일 아침에 조금..\n",
    "    \n",
    "14번 프로젝트를 잘 할 수 있을지 걱정이 된다. 일단 부딪쳐 봐야되겠지.\n",
    "    \n",
    "아마 프로젝트가 변할 수도 있지만 입력 문장 끝에 문장 생성하는 것이 아닌 입력 문장 사이에 문장을 생성하는 것이 있으면 좋겠다고 생각한다.\n",
    "    \n",
    "그러면 내가 쓴 문장에 논리가 안 맞는 것을 찾아서 고쳐써주지 않을까 하는 생각이 들었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
