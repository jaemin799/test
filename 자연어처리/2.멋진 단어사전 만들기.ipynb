{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e757f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "117c870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = list(set(raw))\n",
    "\n",
    "filtered_corpus = [s for s in cleaned_corpus if (len(s) < max_len) & (len(s) >= min_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b85cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 77590 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5079802\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1319\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 77590 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 176795 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 77590\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 241182\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 241182 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=93808 obj=14.8611 num_tokens=530931 num_tokens/piece=5.65976\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=83222 obj=13.5216 num_tokens=533486 num_tokens/piece=6.4104\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=62408 obj=13.5582 num_tokens=554869 num_tokens/piece=8.89099\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=62357 obj=13.5154 num_tokens=555301 num_tokens/piece=8.90519\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46766 obj=13.6977 num_tokens=583543 num_tokens/piece=12.4779\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46766 obj=13.6548 num_tokens=583593 num_tokens/piece=12.479\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35074 obj=13.8914 num_tokens=614352 num_tokens/piece=17.5159\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35074 obj=13.8406 num_tokens=614386 num_tokens/piece=17.5169\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26305 obj=14.1327 num_tokens=646390 num_tokens/piece=24.5729\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26305 obj=14.0776 num_tokens=646424 num_tokens/piece=24.5742\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19728 obj=14.4108 num_tokens=680165 num_tokens/piece=34.4771\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19728 obj=14.3488 num_tokens=680202 num_tokens/piece=34.479\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14796 obj=14.7201 num_tokens=715226 num_tokens/piece=48.3391\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14796 obj=14.6487 num_tokens=715228 num_tokens/piece=48.3393\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11097 obj=15.0839 num_tokens=751803 num_tokens/piece=67.7483\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11097 obj=15.0033 num_tokens=751803 num_tokens/piece=67.7483\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=15.3984 num_tokens=781516 num_tokens/piece=88.8086\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=15.3245 num_tokens=781525 num_tokens/piece=88.8097\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 376725 Jan 15 14:13 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 146094 Jan 15 14:13 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 8000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:\n",
    "        f.write(str(row) + '\\n')\n",
    "        \n",
    "spm.SentencePieceTrainer.Train(\n",
    "'--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size))\n",
    "\n",
    "!ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be6a4e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1255, 11, 304, 7, 3606, 11, 285, 38, 3]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "tokensIDs = s.EncodeAsIds(\"아버지가방에들어가신다.\")\n",
    "print(tokensIDs)\n",
    "\n",
    "print(s.SampleEncodeAsPieces(\"아버지가방에들어가신다\", 1, 0.0))\n",
    "\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218bbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_tokenize(s, corpus):\n",
    "    \n",
    "    tensor = []\n",
    "    \n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "        \n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "        \n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "    \n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split('\\t')[0]\n",
    "        \n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "        \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94c3bf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1957 5607    5    4 7975 2012    3    0    0    0    0    0    0    0]\n",
      " [ 108 1658  101    4    0  470   11    4   14    0 2002    3    3    3]]\n"
     ]
    }
   ],
   "source": [
    "my_corpus = [\"나는 밥을 먹었습니다.\", \"그러나 여전히 ㅠㅠ 배가 고픕니다...\"]\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c4406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
